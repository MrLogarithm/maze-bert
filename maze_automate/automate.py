# Copyright (c) 2018-present, Facebook, Inc.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
#

import torch
import torch.nn as nn
import torch.nn.functional as F
from nltk.tokenize import word_tokenize
from gulardova_code import dictionary_corpus
from gulardova_code.utils import repackage_hidden, batchify, get_batch
import numpy as np
import lexicon_generator
import random
import re
import csv
import os
import sys

import numpy as np
from six.moves import xrange
import tensorflow as tf
from nltk.tokenize import word_tokenize
from google.protobuf import text_format
from one_b_code import data_utils
#### General functions ####
UNIGRAM_FREQ=lexicon_generator.load_unigram('unigram.json')
LEXICON=lexicon_generator.load_lexicon('lexicon.json')

def get_unigram_freq(word):
    '''arguments: word - string
	returns unigram frequency '''
    freq=UNIGRAM_FREQ.get(word)
    if freq!=None:
        return freq
    else: #try different capitalization schemes 
        print (word+" is not in dictionary")
        test_freq=UNIGRAM_FREQ.get(word.lower())
        if test_freq!=None:
            print("Using "+word.lower()+" instead")
            return test_freq
        test_freq=UNIGRAM_FREQ.get(word.capitalize())
        if test_freq!=None:
            print("Using "+word.capitalize()+" instead")
            return test_freq

def strip_end_punct(word):
    '''take a word, return tuple of word without last end punctuation, if any, and end punctuation'''
    if word[-1] in [".",",","!", "?"]:
        return (word[:-1],word[-1])
    return(word, "")

def get_alts(length, freq):
    '''given two numbers (length, frequency), returns a list of words
    with that length and frequency'''
    if length<3: #adjust length if needed
        length=3
    if length>15:
        length=15
    alts=LEXICON.get((length, freq))
    if alts==None:
        print("Trouble finding words with length "+str(length)+ " and frequency "+str(freq))
    else:
        random.shuffle(alts)
    return alts

def get_alt_nums(word_list):
    ''' given a list of words, returns the average length and average frequency as a tuple'''
    length=0
    freq=0
    for w in range(len(word_list)): #find individual length, freq
        word=strip_end_punct(word_list[w])[0]
        length+=len(word)
        #print(type(freq))
        freq+=get_unigram_freq(word)
    avg_length=round(length/len(word_list)) #take avg and round
    avg_freq=round(freq/len(word_list))
    return(avg_length, avg_freq)

def save_output(outfile,item_to_info, end_result):
    '''given the location of a file to write to, item_to_info as generated by read_input, and results as generated from language model, writes a list to output
    specifications:
    item_to_info is a dictionary of item numbers as keys and a pair of lists (of conditions and of sentences) as value 
    end_result has a grouped list of sentence formatted distractor words
    will write a semicolon delimited file with 
    column 1 = "tag"/condition copied over from item_to_info (from input file)
    column 2 = item number
    column 3 = good sentence
    column 4 = string of distractor words in order. '''
    with open(outfile, 'w') as f:
        for key in item_to_info:
            for i in range(len(item_to_info[key][1])):
                f.write('"'+item_to_info[key][0][i]+'";')
                f.write('"'+key+'";')
                f.write('"'+item_to_info[key][1][i]+'";')
                f.write('"'+end_result[item_to_info[key][1][i]]+'"\n')

def save_ibex_format(outfile, item_to_info, end_result):
 '''given the location of a file to write to, item_to_info as generated by read_input, and results as generated from language model, writes a list to output
    specifications:
    item_to_info is a dictionary of item numbers as keys and a pair of lists (of conditions and of sentences) as value 
    end_result has a grouped list of sentence formatted distractor words
    will write output suitable for copying in the items list of an ibex Maze experiment file'''
    with open(outfile, 'w') as f:
        for key in item_to_info:
            for i in range(len(item_to_info[key][1])):
                f.write('[["'+item_to_info[key][0][i]+'", ')
                f.write(key+'], "Maze", {s:')
                f.write('"'+item_to_info[key][1][i]+'", a:')
                f.write('"'+end_result[item_to_info[key][1][i]]+'"}],\n')

def read_input(filename):
    '''file should be semicolon delimited
    first column "tag" (any info that should stay associated with the sentence such as condition etc (it will be copied to eventual output),
    second column item number (sentences that share item num will get same distractors, and *Must* have same # of words,
    third column sentence
    returns item_to_info = item_to_info is a dictionary of item numbers as keys and a pair of lists (of conditions and of sentences) as value , sentences is a list of sentences grouped by item number (ie will get matching distractors)'''
    item_to_info={}
    with open(filename, 'r') as tsv:
        f = csv.reader(tsv, delimiter=";", quotechar='"')
        for row in f:
            if row[1] in item_to_info: #item num already seen
                item_to_info[row[1]][0].append(row[0]) #add condition to the list
                item_to_info[row[1]][1].append(row[2].strip()) #add sentence to the list
            else:
                item_to_info[row[1]]=[[row[0]],[row[2].strip()]] # new item num, add a new entry 
    sentences=[]
    for item in sorted(item_to_info):
        sentences.append(item_to_info[item][1]) #make a list of lists of sentences, grouped by item number
    return (item_to_info, sentences)

def mainish(infile, outfile, lang_model="gula", out_format="basic"):
    '''does everything
    infile= where input is
    outfile = where to write output to
    lang_model = either "gula" or "one_b" for which language model to use
    out_format = either "basic" (for a semicolon delimited output) or "ibex" for an ibex ready output
    no return value'''
    item_to_info, sentences=read_input(infile) # read input
    end_result={}
    if lang_model=="gula": #for gula
        model, device =load_model_gula() #set up 
        dictionary, ntokens = load_dict_gula()
        for i in range(len(sentences)): #process all the sentences
            bad=do_sentence_set_gula(sentences[i], model, device, dictionary, ntokens)
            for j in range(len(sentences[i])): #record results
                end_result[sentences[i][j]]=bad
    elif lang_model=="one_b": #for one b model
        sess, t =load_model_one_b() #set up
        vocab=load_dict_one_b()
        for i in range(len(sentences)): #process all the sentences
            bad=do_sentence_set_one_b(sentences[i],sess, t, vocab)
            for j in range(len(sentences[i])): #record results
                end_result[sentences[i][j]]=bad
    if out_format=="ibex": #save output
        save_ibex_format(outfile, item_to_info, end_result)
    else: #save output
        save_output(outfile,item_to_info, end_result)

#### Gulardova specific ####
def load_model_gula():
    '''sets up a model for gulordava, returns the model and device'''
    with open("gulardova_data/hidden650_batch128_dropout0.2_lr20.0.pt", 'rb') as f:
        print("Loading the model")
        # to convert model trained on cuda to cpu model
        model = torch.load(f, map_location = lambda storage, loc: storage) #read the serialized model into an object
    model.eval() #put model in eval mode
    model.cpu() # put it on cpu 
    device = torch.device("cpu") #what sort of data model is stored on
    return (model, device)

def load_dict_gula():
    '''loads the dictionary, returns dictionary and length of dictionary'''
    dictionary = dictionary_corpus.Dictionary("gulardova_data") #load a dictionary
    ntokens = dictionary.__len__() #length of dictionary
    return(dictionary, ntokens)

def new_sentence_gula(model, device, ntokens):
    '''sets up a new sentence for gulordava model
    inputs: model, device as from load_model, ntokens=length of dictionary (from load_dict)
    returns a word placeholder and an initialized hidden layer'''
    hidden = model.init_hidden(1) #sets initial values on hidden layer 
    input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device) #gets a random wordid from vocab list
    return (input, hidden)

def update_sentence_gula(word, input, model, hidden, dictionary):
    '''takes in a sentence so far adds the next word and returns the new list of surprisals
    inputs: word = next word in sentence, input= placeholder for wordid, model= model from load_model, hidden = hidden layer representation of sentence so far, dictionary= dictionary from load_dict
    returns: output = (probably not needed??), hidden= new hidden layer, word_surprisals = distribution of surprisals for all words'''
    parts=word_tokenize(word) #get list of tokens
    for part in parts:
       token = dictionary_corpus.tokenize_str(dictionary,part)[0] #get id of token
       if part not in dictionary.word2idx:
            print("Good word "+part+" is unknown") #error message
       input.fill_(token.item()) #fill with value of token
       output, hidden = model(input,hidden) #do the model thing       
       word_weights = output.squeeze().div(1.0).exp().cpu() #process output into weights
       word_surprisals = -1*torch.log2(word_weights/sum(word_weights))# turn into surprisals
    return (output, hidden, word_surprisals)

def get_surprisal_gula(surprisals, dictionary, word):
    '''given the a surprisal distribution, a dictionary (word to word ids), and a word
    returns the numeric surprisal value of the word, if word is unknown returns -1
    We don't trust surprisal values for UNK words'''
    token=dictionary_corpus.tokenize_str(dictionary, word)[0] #take first token of word
    if word not in dictionary.word2idx:
        print(word+" is unknown")
        return(-1) #use -1 as an error code
    else:
        return surprisals[token].item() #numeric value of word's surprisal

def find_bad_enough_gula(num_to_test, minimum, word_list, surprisals_list, dictionary):
    '''will return the word that is at least minimum surprisal for all sentences. if goes through num_to_test words
    without finding a bad enough, returns the worst it's seen (worst= highest min)
    inputs: num_to_test - an integer for how many candidate words to try,
    minimum = minimum suprisal to look for (will return first word with at least this surprisal for all sentences)
    word_list = the good words that occur in this position
    surprisals_list = distribution of surprisals (from update_sentence)
    dictionary = word to word_id lok up
    returns: a word that meets the surprisal target or the best option if num_to_test have been tested and none have met minimum'''    
    best_word=""
    best_surprisal=0
    length, freq=get_alt_nums(word_list) # find what length and frequency is average for good words
    options_list=None
    i=0
    k=0
    while options_list==None:
        options_list=get_alts(length, freq+i) # find words with that length and frequency 
        i+=1 #if there weren't any, try a slightly higher frequency
    while best_surprisal==0 or k<num_to_test: #no word has a real surprisal value or we haven't tested enough
        min_surprisal=100 #dummy value higher than we expect any surprisal to actually be
        if k==len(options_list): # if we run out of options
            new_options=None
            while new_options==None:
                new_options=get_alts(length, freq+i) #keep trying to find higher frequency words
                i+=1
            options_list.extend(new_options) #add to the options list
        word=options_list[k] #try the kth option
        k+=1
        for j in range(len(surprisals_list)): # for each sentence this word needs to fit
            surprisal=get_surprisal_gula(surprisals_list[j], dictionary, word) #find that word on the list
            min_surprisal=min(min_surprisal, surprisal) #lowest surprisal we've seen so far in this sentence list 
        if min_surprisal>=minimum: #if surprisal in each condition is greater than required
            return (word) # we found a word to use and are done here
        elif min_surprisal>best_surprisal: #if it's the best option so far, record that
            best_word=word
            best_surprisal=min_surprisal
    print("Couldn't meet surprisal target, returning with surprisal of "+str(best_surprisal)) # if we've run through our list, return the best we have, but warn about it
    return(best_word)
            

def do_sentence_set_gula(sentence_set, model, device, dictionary, ntokens):
    '''processes a set of sentences that should all get the same distractors, returns the distractors in sentence form
    arguments: sentence_set= a list of sentences (all equal length)
    model, device = from load_model
    dictionary, ntokens = from load_dictionary
    returns a sentence format string of the distractors'''
    bad_words=["x-x-x"]
    words=[]
    sentence_length=len(sentence_set[0].split()) #find length of first item 
    for i in range(len(sentence_set)):
        sentence=sentence_set[i] 
        sent_words=sentence.split()
        if len(sent_words)!=sentence_length:
            print("inconsistent lengths!!")  #complain if they aren't the same length
        words.append(sent_words) # make a new list with the sentences as word lists
    hidden=[None]*len(sentence_set) #set up a bunch of stuff
    input=[None]*len(sentence_set)
    output=[None]*len(sentence_set)
    surprisals=[None]*len(sentence_set)
    for i in range(len(sentence_set)): # more set up
        input[i], hidden[i]=new_sentence_gula(model, device, ntokens)
    for j in range(sentence_length-1): # for each word position in the sentence
        word_list=[]
        for i in range(len(sentence_set)): # for each sentence
            output[i], hidden[i], surprisals[i] = update_sentence_gula(words[i][j], input[i], model, hidden[i], dictionary) # and the next word to the sentence
            word_list.append(words[i][j+1]) #add the word after that to a list of words
        bad_word=find_bad_enough_gula(100, 21, word_list, surprisals, dictionary) #find an alternate word using the surprisals and matching frequency for the good words; try 100 words, aim for surprisal of 21 or higher
        cap=word_list[0][0].isupper() #is the first letter of the first of the "good" words capitalized?
        if cap: # if it is, adjust capitalization of the distractor to match
            bad_word=bad_word[0].upper()+bad_word[1:]
        bad_word=bad_word+strip_end_punct(word_list[0])[1] # if the good word had end punctuation, append the same end punctuation to the bad word
        bad_words.append(bad_word) # add the fixed bad word to a running list
    bad_sentence=" ".join(bad_words) # turn the list of distractors into sentence format
    return(bad_sentence) # and return 

#### One b specific ####
def load_model_one_b():
    """Load the model from GraphDef and Checkpoint.
    TensorFlow session and tensors dict.
    """
    with tf.Graph().as_default():
    
        with tf.gfile.FastGFile("one_b_data/graph-2016-09-10.pbtxt", 'r') as f:
            s = f.read()
            gd = tf.GraphDef()
            text_format.Merge(s, gd)

        tf.logging.info('Recovering Graph %s', "one_b_data/graph-2016-09-10.pbtxt")
        t = {}
        [t['states_init'], t['lstm/lstm_0/control_dependency'],
         t['lstm/lstm_1/control_dependency'], t['softmax_out'], t['class_ids_out'],
         t['class_weights_out'], t['log_perplexity_out'], t['inputs_in'],
         t['targets_in'], t['target_weights_in'], t['char_inputs_in'],
         t['all_embs'], t['softmax_weights'], t['global_step']
        ] = tf.import_graph_def(gd, {}, ['states_init',
                                         'lstm/lstm_0/control_dependency:0',
                                         'lstm/lstm_1/control_dependency:0',
                                         'softmax_out:0',
                                         'class_ids_out:0',
                                         'class_weights_out:0',
                                         'log_perplexity_out:0',
                                         'inputs_in:0',
                                         'targets_in:0',
                                         'target_weights_in:0',
                                         'char_inputs_in:0',
                                         'all_embs_out:0',
                                         'Reshape_3:0',
                                         'global_step:0'], name='')

        sys.stderr.write('Recovering checkpoint %s\n' % "one_b_data/ckpt-*")
        sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))
        sess.run('save/restore_all', {'save/Const:0': "one_b_data/ckpt-*"})
        sess.run(t['states_init'])
    return (sess, t)            

def load_dict_one_b():
    '''get the dictionary used by one_b'''
    vocab=data_utils.CharsVocabulary("one_b_data/vocab-2016-09-10.txt", 50)
    return(vocab)

def new_sentence_one_b(vocab):
    '''initializes a new blank sentence'''
    targets = np.zeros([1, 1], np.int32)
    weights = np.ones([1, 1], np.float32)
    inputs = np.zeros([1, 1], np.int32)
    char_ids_inputs = np.zeros( [1, 1, vocab.max_word_length], np.int32)
    return (targets, weights, inputs, char_ids_inputs)       

def tokenize_one_b(word):
    '''takes a word, returns it split into tokens to match tokenization that one_b model expects'''
    new_string=re.sub("([.,?!])", r" \1 ", word) # split some punctuation as their own words
    newer_string=re.sub("'", " '", new_string) # split words before apostrophes
    tokens=newer_string.split()
    return tokens
    
def update_sentence_one_b(word, inputs, char_ids_inputs, sess, t, targets, weights, vocab):
    '''takes in a sentence so far adds the next word and returns the new list of surprisals
    inputs: word = next word in sentence, inputs= placeholder for word, char_ids_inputs = placeholder for word's characters, sess, t from load_model, targets, weights are describing the sentence so far, vocab is word to id lookup
    returns: targets = output sentence, weights = output form sentence, softmax = distribution over next word surprisals'''
    parts=tokenize_one_b(word) #get list of tokens
    for part in parts:
        token = vocab.word_to_id(part)#get id of token
        char_tokens=vocab.word_to_char_ids(part) #get char ids
        if token==vocab.unk:
            print("Good word "+part+" is unknown") #error message
        inputs[0,0]=token
        char_ids_inputs[0,0,:]=char_tokens
        softmax = sess.run(t['softmax_out'], # run the model
                                     feed_dict={t['char_inputs_in']: char_ids_inputs,
                                                t['inputs_in']: inputs,
                                                t['targets_in']: targets,
                                                t['target_weights_in']: weights})
    return (targets, weights, softmax) # not sure we need to return targets

def get_surprisal_one_b(softmax, vocab, word):
    '''given the surprisal distribution, a vocab (word to word ids), and a word
    returns the numeric surprisal value of the word, if word is unknown returns -1
    We don't trust surprisal values for UNK words'''
    token=vocab.word_to_id(word) #take first token of word
    if token ==vocab.unk:
        print(word+" is unknown")
        return(-1) #use -1 as an error code
    else:
        return -1 * np.log2(softmax[0][token]) #numeric value of word's surprisal

def find_bad_enough_one_b(num_to_test, minimum, word_list, surprisals_list, vocab):
    '''will return the word that is at least minimum surprisal for all sentences. if goes through num_to_test words
    without finding a bad enough, returns the worst it's seen (worst= highest min)
    inputs: num_to_test - an integer for how many candidate words to try,
    minimum = minimum suprisal to look for (will return first word with at least this surprisal for all sentences)
    word_list = the good words that occur in this position
    surprisals_list = distribution of probabilities (from update_sentence)
    vocab = word to word id lookup
    returns: a word that meets the surprisal target or the best option if num_to_test have been tested and none have met minimum'''     
    best_word=""
    best_surprisal=0
    length, freq=get_alt_nums(word_list) # find what length and frequency is average for good words
    options_list=None
    i=0
    k=0
    while options_list==None:
        options_list=get_alts(length, freq+i) # find words with that length and frequency 
        i+=1 #if there weren't any, try a slightly higher frequency
    while best_surprisal==0 or k<num_to_test: #no word has a real surprisal value or we haven't tested enough
        min_surprisal=100 #dummy value higher than we expect any surprisal to actually be
        if k==len(options_list): # if we run out of options
            new_options=None
            while new_options==None:
                new_options=get_alts(length, freq+i) #keep trying to find higher frequency words
                i+=1
            options_list.extend(new_options) #add to the options list
        word=options_list[k] #try the kth option
        k+=1
        for j in range(len(surprisals_list)): # for each sentence this word needs to fit
            surprisal=get_surprisal_one_b(surprisals_list[j], dictionary, word) #find that word on the list
            min_surprisal=min(min_surprisal, surprisal) #lowest surprisal we've seen so far in this sentence list 
        if min_surprisal>=minimum: #if surprisal in each condition is greater than required
            return (word) # we found a word to use and are done here
        elif min_surprisal>best_surprisal: #if it's the best option so far, record that
            best_word=word
            best_surprisal=min_surprisal
    print("Couldn't meet surprisal target, returning with surprisal of "+str(best_surprisal)) # if we've run through our list, return the best we have, but warn about it
    return(best_word)

def do_sentence_set_one_b(sentence_set, sess, t, vocab):
    '''processes a set of sentences that should all get the same distractors, returns the distractors in sentence form
    arguments: sentence_set= a list of sentences (all equal length)
    sess, t = from load_model
    vocab = from load_dictionary
    returns a sentence format string of the distractors'''
    bad_words=["x-x-x"]
    words=[]
    sentence_length=len(sentence_set[0].split()) # length of each sentence
    for i in range(len(sentence_set)):
        sentence=sentence_set[i] 
        sent_words=sentence.split() # turn sentence into words
        if len(sent_words)!=sentence_length: # complain if there are inconsistent lengths
            print("inconsistent lengths!!")  
        words.append(sent_words) # make a list of list of words
    inputs=[None]*len(sentence_set) # initialize a bunch of things
    char_ids_inputs=[None]*len(sentence_set)
    weights=[None]*len(sentence_set)
    targets=[None]*len(sentence_set)
    softmaxes=[None]*len(sentence_set)
    for i in range(len(sentence_set)):
        targets[i], weights[i], inputs[i], char_ids_inputs[i]=new_sentence_one_b(vocab)
    for j in range(sentence_length-1): # for each word position
        word_list=[]
        for i in range(len(sentence_set)): # for each sentence
            targets[i], weights[i], softmaxes[i] = update_sentence_one_b(words[i][j], inputs[i], char_ids_inputs[i], sess, t, targets[i], weights[i], vocab) # add a word to the sentence
            word_list.append(words[i][j+1]) # make a list of the 'good' next words
        bad_word=find_bad_enough_one_b(100, 21, word_list, softmaxes, vocab) # for a distractor word for that position, using the probability distributions and the good words. Try 100 options, aim for surprisal of at least 21
        cap=word_list[0][0].isupper() #is the first character of the first good word capitalized
        if cap: #make distractor word match capitalization
            bad_word=bad_word[0].upper()+bad_word[1:] 
        bad_word=bad_word+strip_end_punct(word_list[0])[1] #make distractor work match end punctuation of good word
        bad_words.append(bad_word) # add distractor to list
    bad_sentence=" ".join(bad_words) # turn list of distractors into a sentence
    return(bad_sentence)
#####

mainish("test_input.txt", "output_test.txt", "one_b", "ibex")   

