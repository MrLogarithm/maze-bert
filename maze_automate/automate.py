import gulordava

import numpy as np
import lexicon_generator
import random
import re
import csv
import os
import sys


#### General functions ####
UNIGRAM_FREQ=lexicon_generator.load_unigram('unigram.json')
LEXICON=lexicon_generator.load_lexicon('lexicon.json')

def get_unigram_freq(word):
    '''arguments: word - string
	returns unigram frequency '''
    freq=UNIGRAM_FREQ.get(word)
    if freq!=None:
        return freq
    else: #try different capitalization schemes 
        print (word+" is not in dictionary")
        test_freq=UNIGRAM_FREQ.get(word.lower())
        if test_freq!=None:
            print("Using "+word.lower()+" instead")
            return test_freq
        test_freq=UNIGRAM_FREQ.get(word.capitalize())
        if test_freq!=None:
            print("Using "+word.capitalize()+" instead")
            return test_freq

def strip_end_punct(word):
    '''take a word, return tuple of word without last end punctuation, if any, and end punctuation'''
    if word[-1] in [".",",","!", "?"]:
        return (word[:-1],word[-1])
    return(word, "")

def get_alts(length, freq):
    '''given two numbers (length, frequency), returns a list of words
    with that length and frequency'''
    if length<3: #adjust length if needed
        length=3
    if length>15:
        length=15
    alts=LEXICON.get((length, freq))
    if alts==None:
        print("Trouble finding words with length "+str(length)+ " and frequency "+str(freq))
    else:
        random.shuffle(alts)
    return alts

def get_alt_nums(word_list):
    ''' given a list of words, returns the average length and average frequency as a tuple'''
    length=0
    freq=0
    for w in range(len(word_list)): #find individual length, freq
        word=strip_end_punct(word_list[w])[0]
        length+=len(word)
        #print(type(freq))
        freq+=get_unigram_freq(word)
    avg_length=round(length/len(word_list)) #take avg and round
    avg_freq=round(freq/len(word_list))
    return(avg_length, avg_freq)

def save_output(outfile,item_to_info, end_result):
    '''given the location of a file to write to, item_to_info as generated by read_input, and results as generated from language model, writes a list to output
    specifications:
    item_to_info is a dictionary of item numbers as keys and a pair of lists (of conditions and of sentences) as value 
    end_result has a grouped list of sentence formatted distractor words
    will write a semicolon delimited file with 
    column 1 = "tag"/condition copied over from item_to_info (from input file)
    column 2 = item number
    column 3 = good sentence
    column 4 = string of distractor words in order. '''
    with open(outfile, 'w') as f:
        for key in item_to_info:
            for i in range(len(item_to_info[key][1])):
                f.write('"'+item_to_info[key][0][i]+'";')
                f.write('"'+key+'";')
                f.write('"'+item_to_info[key][1][i]+'";')
                f.write('"'+end_result[item_to_info[key][1][i]]+'"\n')

def save_ibex_format(outfile, item_to_info, end_result):
    '''given the location of a file to write to, item_to_info as generated by read_input, and results as generated from language model, writes a list to output
    specifications:
    item_to_info is a dictionary of item numbers as keys and a pair of lists (of conditions and of sentences) as value 
    end_result has a grouped list of sentence formatted distractor words
    will write output suitable for copying in the items list of an ibex Maze experiment file'''
    with open(outfile, 'w') as f:
        for key in item_to_info:
            for i in range(len(item_to_info[key][1])):
                f.write('[["'+item_to_info[key][0][i]+'", ')
                f.write(key+'], "Maze", {s:')
                f.write('"'+item_to_info[key][1][i]+'", a:')
                f.write('"'+end_result[item_to_info[key][1][i]]+'"}],\n')

def read_input(filename):
    '''file should be semicolon delimited
    first column "tag" (any info that should stay associated with the sentence such as condition etc (it will be copied to eventual output),
    second column item number (sentences that share item num will get same distractors, and *Must* have same # of words,
    third column sentence
    returns item_to_info = item_to_info is a dictionary of item numbers as keys and a pair of lists (of conditions and of sentences) as value , sentences is a list of sentences grouped by item number (ie will get matching distractors)'''
    item_to_info={}
    with open(filename, 'r') as tsv:
        f = csv.reader(tsv, delimiter=";", quotechar='"')
        for row in f:
            if row[1] in item_to_info: #item num already seen
                item_to_info[row[1]][0].append(row[0]) #add condition to the list
                item_to_info[row[1]][1].append(row[2].strip()) #add sentence to the list
            else:
                item_to_info[row[1]]=[[row[0]],[row[2].strip()]] # new item num, add a new entry 
    sentences=[]
    for item in sorted(item_to_info):
        sentences.append(item_to_info[item][1]) #make a list of lists of sentences, grouped by item number
    return (item_to_info, sentences)

def mainish(infile, outfile, lang_model="gulordava", out_format="basic"):
    '''does everything
    infile= where input is
    outfile = where to write output to
    lang_model = either "gulordava" or "one_b" for which language model to use
    out_format = either "basic" (for a semicolon delimited output) or "ibex" for an ibex ready output
    no return value'''
    item_to_info, sentences=read_input(infile) # read input
    end_result={}
    if lang_model=="gulordava": #for gulordava
        model, device =gulordava.load_model_gulordava() #set up 
        dictionary, ntokens = gulordava.load_dict_gulordava()
        for i in range(len(sentences)): #process all the sentences
            bad=gulordava.do_sentence_set_gulordava(sentences[i], model, device, dictionary, ntokens)
            for j in range(len(sentences[i])): #record results
                end_result[sentences[i][j]]=bad
    elif lang_model=="one_b": #for one b model
        import one_b
        sess, t =one_b.load_model_one_b() #set up
        vocab=one_b.load_dict_one_b()
        for i in range(len(sentences)): #process all the sentences
            bad=one_b.do_sentence_set_one_b(sentences[i],sess, t, vocab)
            for j in range(len(sentences[i])): #record results
                end_result[sentences[i][j]]=bad
    if out_format=="ibex": #save output
        save_ibex_format(outfile, item_to_info, end_result)
    else: #save output
        save_output(outfile,item_to_info, end_result)



mainish("test_input.txt", "output_test.txt", "gulordava", "ibex")   

